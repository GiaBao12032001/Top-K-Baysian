# -*- coding: utf-8 -*-
"""algorithmn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i5zFqxRe5SetA8BekBIj3R8qgBovBWff
"""

import math, heapq


def expected_support(itemset, data):
    """Expected support: sum of products of probabilities of items in each transaction."""
    total = 0.0
    for t in data:
        prod = 1.0
        for item in itemset:
            prod *= t.get(item, 0.0)
        total += prod
    return total

def has_cycle(nodes, edges):
    """Return True if graph has a cycle (False if acyclic)."""
    children = {n: [] for n in nodes}
    for (u,v) in edges:
        children[u].append(v)
    visited, stack = set(), set()
    def dfs(u):
        visited.add(u); stack.add(u)
        for v in children.get(u, []):
            if v not in visited:
                if dfs(v): return True
            elif v in stack:
                return True
        stack.remove(u)
        return False
    for n in nodes:
        if n not in visited:
            if dfs(n):
                return True
    return False

def is_dag(nodes, edges):
    return not has_cycle(nodes, edges)

def compute_score(nodes, edges, data, weight_support,marginals):
    """Compute composite score = BIC + weight_support * sum(expected supports of (child+parents) sets)."""
    # Build parent lists from edges
    parents = {n: [] for n in nodes}
    for (u,v) in edges:
        parents[v].append(u)

    # Sum expected supports of each child+parent set
    support_sum = 0.0
    for node in nodes:
        if parents[node]:
            itemset = set(parents[node] + [node])
            support_sum += expected_support(itemset, data)
    for node in nodes:
      if len(parents[node]) > 2:
        return -float("inf")

    # Compute log-likelihood and parameter count for BIC
    loglike = 0.0
    N = len(data)
    param_count = 0
    for node in nodes:
        P = parents[node]
        if P:
            # all binary configs of parents
            for bits in range(2**len(P)):
                total_count = count_X1 = 0.0
                for t in data:
                    prob = 1.0
                    for i,p in enumerate(P):
                        if (bits >> i) & 1:
                            prob *= t.get(p, 0.0)
                        else:
                            prob *= (1 - t.get(p, 0.0))
                    total_count += prob
                    count_X1 += prob * t.get(node, 0.0)
                count_X0 = total_count - count_X1
                if total_count > 0:
                    param_count += 1
                    # add contributions to log-likelihood
                    if count_X1 > 0:
                        loglike += count_X1 * math.log(count_X1/total_count)
                    if count_X0 > 0:
                        loglike += count_X0 * math.log(count_X0/total_count)
        else:
            # Node with no parents
            total_count = float(N)
            count_X1 = marginals[node]
            count_X0 = total_count - count_X1
            if total_count > 0:
                param_count += 1
                if count_X1 > 0:
                    loglike += count_X1 * math.log(count_X1/total_count)
                if count_X0 > 0:
                    loglike += count_X0 * math.log(count_X0/total_count)

    # BIC score: log-likelihood minus (log(N)/2)*param_count (see :contentReference[oaicite:11]{index=11})
    bic = loglike - (math.log(N)/2.0) * param_count

    return bic + weight_support * support_sum

def top_k_bayesian_networks(data, min_support, K, weight_support, sample_size):
    if sample_size < len(data):
        import random
        data = random.sample(data, sample_size)
    """Return top-K BN structures (as edge-sets) by combined support+score."""
    # 1. Collect items and candidate edges (frequent pairs)
    nodes = sorted({ x for t in data for x in t.keys() })
    marginals = {node: sum(t.get(node, 0.0) for t in data) for node in nodes}
    edges_candidates = []
    for i in range(len(nodes)):
        for j in range(i+1, len(nodes)):
            sup = expected_support({nodes[i], nodes[j]}, data)
            if sup >= min_support:
                edges_candidates.append((nodes[i], nodes[j]))
                edges_candidates.append((nodes[j], nodes[i]))

    # 2. Best-first search initialization
    initial_edges = frozenset()   # start with empty graph
    initial_score = compute_score(nodes, initial_edges, data, weight_support, marginals)
    heap = [(-initial_score, initial_edges)]
    visited = set()
    top_networks = []

    # 3. Search loop
    MAX_EXPANSIONS = 20  # Limit total DAG expansions
    loop_count = 0

    while heap and len(top_networks) < K and loop_count < MAX_EXPANSIONS:
      loop_count += 1
      negscore, edges = heapq.heappop(heap)
      score = -negscore
      print(f"Expanded {loop_count} DAGs... found {len(top_networks)} so far.")
      if edges in visited:
          continue
      visited.add(edges)
      top_networks.append((edges, score))

        # expand by adding one new edge from candidates
      for e in edges_candidates:
          if e in edges:
               continue
          new_edges = set(edges)
          new_edges.add(e)
          if is_dag(nodes, new_edges):
            new_edges = frozenset(new_edges)
            if new_edges not in visited:
                  new_score = compute_score(nodes, new_edges, data, weight_support,marginals)
                  heapq.heappush(heap, (-new_score, new_edges))

    return top_networks