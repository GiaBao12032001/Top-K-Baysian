# -*- coding: utf-8 -*-
"""Untitled42.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F-u_Me3iLFUziRUfsr30PTUB4ng_oS3i
"""

# Install mlxtend first if you don't have it
!pip install mlxtend

from mlxtend.frequent_patterns import apriori, association_rules
import pandas as pd

# Sample transaction data
data = {
    'milk': [1, 1, 0, 1, 1],
    'bread': [1, 1, 1, 1, 0],
    'butter': [0, 1, 1, 1, 1],
    'cheese': [1, 0, 1, 0, 1]
}

df = pd.DataFrame(data)

# Apply Apriori algorithm
frequent_itemsets = apriori(df, min_support=0.5, use_colnames=True)

# Generate association rules
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)

print("Frequent Itemsets:\n", frequent_itemsets)
print("\nAssociation Rules:\n", rules)
import pandas as pd

# Sample transaction data
data = {
    'milk': [1, 1, 0, 1, 1],
    'bread': [1, 1, 1, 1, 0],
    'butter': [0, 1, 1, 1, 1],
    'cheese': [1, 0, 1, 0, 1]
}

# Convert the values to boolean
df = pd.DataFrame(data).astype(bool)

# Apply Apriori algorithm
from mlxtend.frequent_patterns import apriori, association_rules

frequent_itemsets = apriori(df, min_support=0.2, use_colnames=True)

# Generate association rules with adjusted lift threshold
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1.0)

print("Frequent Itemsets:\n", frequent_itemsets)
print("\nAssociation Rules:\n", rules)

!pip install pgmpy --upgrade

!pip install pgmpy

import pandas as pd
import requests
from io import StringIO
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder
url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'
column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',
                'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss',
                'hours-per-week', 'native-country', 'income']
response = requests.get(url)
data = response.text
df = pd.read_csv(StringIO(data), header=None, names=column_names, na_values='?')
print("Dataset Preview:")
print(df.head())
df_processed = df[['workclass', 'education', 'marital-status', 'occupation', 'relationship',
                   'race', 'sex', 'native-country']].apply(lambda x: x.str.strip())
df_encoded = pd.get_dummies(df_processed)
frequent_itemsets = apriori(df_encoded, min_support=0.02, use_colnames=True)
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1.0)
print("\nFrequent Itemsets:")
print(frequent_itemsets)

print("\nAssociation Rules:")
print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

import pandas as pd
from mlxtend.frequent_patterns import apriori, association_rules

# Sample transaction data (using integers)
data = {
    'milk': [1, 1, 0, 1, 1],
    'bread': [1, 1, 1, 1, 0],
    'butter': [0, 1, 1, 1, 1],
    'cheese': [1, 0, 1, 0, 1]
}

# Convert to DataFrame and ensure boolean type (avoids warning)
df = pd.DataFrame(data).astype(bool)

# Apply Apriori with lower min_support to get more frequent itemsets
frequent_itemsets = apriori(df, min_support=0.2, use_colnames=True)

# Generate association rules (using confidence instead of lift for more results)
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.2)

print("Frequent Itemsets:\n", frequent_itemsets)
print("\nAssociation Rules:\n", rules)

from mlxtend.frequent_patterns import fpgrowth, association_rules
import pandas as pd

data = {
    'milk': [1, 1, 0, 1, 1],
    'bread': [1, 1, 1, 1, 0],
    'butter': [0, 1, 1, 1, 1],
    'cheese': [1, 0, 1, 0, 1]
}

df = pd.DataFrame(data).astype(bool)

frequent_itemsets = fpgrowth(df, min_support=0.1, use_colnames=True)

rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)

print("Frequent Itemsets:\n", frequent_itemsets)
print("\nAssociation Rules:\n", rules)

from mlxtend.frequent_patterns import apriori, association_rules
import pandas as pd

# Sample transaction data
data = {
    'milk': [1, 1, 0, 1, 1],
    'bread': [1, 1, 1, 1, 0],
    'butter': [0, 1, 1, 1, 1],
    'cheese': [1, 0, 1, 0, 1]
}

df = pd.DataFrame(data)

# Apply Apriori algorithm
frequent_itemsets = apriori(df, min_support=0.5, use_colnames=True)

# Generate association rules
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)

print("Frequent Itemsets:\n", frequent_itemsets)
print("\nAssociation Rules:\n", rules)

from mlxtend.frequent_patterns import fpgrowth, association_rules
import pandas as pd

# Sample transaction data
data = {
    'milk': [1, 1, 0, 1, 1],
    'bread': [1, 1, 1, 1, 0],
    'butter': [0, 1, 1, 1, 1],
    'cheese': [1, 0, 1, 0, 1]
}

df = pd.DataFrame(data)

# Apply FP-Growth algorithm
frequent_itemsets = fpgrowth(df, min_support=0.5, use_colnames=True)

# Generate association rules
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)

print("Frequent Itemsets:\n", frequent_itemsets)
print("\nAssociation Rules:\n", rules)

transactions = [
    {'milk', 'bread', 'butter'},
    {'bread', 'butter', 'cheese'},
    {'milk', 'bread', 'cheese'},
    {'milk', 'butter'},
    {'bread', 'butter', 'cheese'}
]

# Convert transactions to a vertical data format (TID-lists)
item_tids = {}
for tid, transaction in enumerate(transactions):
    for item in transaction:
        if item not in item_tids:
            item_tids[item] = set()
        item_tids[item].add(tid)

# Function to perform Eclat and find frequent itemsets
def eclat(item_tids, min_support=2):
    frequent_itemsets = {}

    def eclat_recursive(prefix, tids, k):
        if len(tids) >= min_support:
            frequent_itemsets[frozenset(prefix)] = tids

        # Optimization: Only consider items lexicographically larger than the last item in prefix
        # to avoid generating duplicate itemsets
        last_item = prefix[-1] if prefix else None

        for item, item_tid in item_tids.items():
            if last_item and item <= last_item:  # Lexicographical pruning
                continue

            # Optimization: Compute intersection using set intersection (&)
            # which is generally faster than `issubset` for large sets
            intersection_tids = tids & item_tid
            if len(intersection_tids) >= min_support:
                eclat_recursive(prefix + [item], intersection_tids, k + 1)

    # Start Eclat from single items
    for item, tids in item_tids.items():
        eclat_recursive([item], tids, 1)

    return frequent_itemsets

# Run Eclat to find frequent itemsets
frequent_itemsets = eclat(item_tids, min_support=2)

print("Frequent Itemsets (Eclat):\n", frequent_itemsets)

import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori
transactions = [
    ['A', 'B', 'C'],
    ['A', 'B'],
    ['B', 'C'],
    ['A', 'C'],
    ['B', 'C', 'D'],
    ['A', 'D'],
    ['C', 'D'],
]
te = TransactionEncoder()
te_ary = te.fit(transactions).transform(transactions)
df_bool = pd.DataFrame(te_ary, columns=te.columns_).astype(bool)
freq_itemsets = apriori(df_bool, min_support=0.3, use_colnames=True)
freq_itemsets['length'] = freq_itemsets['itemsets'].apply(len)

K = 3
top_k_itemsets = freq_itemsets.sort_values('support', ascending=False).head(K)
print("Top-K Frequent Itemsets:\n", top_k_itemsets)

import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori
from pgmpy.estimators import HillClimbSearch, BIC
from pgmpy.models import BayesianNetwork
transactions = [
    ['A','B','C'], ['A','B'], ['B','C'], ['A','C'],
    ['B','C','D'], ['A','D'], ['C','D']
]
te = TransactionEncoder()
df_bool = pd.DataFrame(te.fit(transactions).transform(transactions),
                       columns=te.columns_).astype(bool)
freq_itemsets = apriori(df_bool, min_support=0.3, use_colnames=True)
K = 3
top_k = freq_itemsets.sort_values('support', ascending=False).head(K)
data_int = df_bool.astype(int)
results = []
for _, row in top_k.iterrows():
    items = list(row['itemsets'])
    subdata = data_int[items]

    hc = HillClimbSearch(subdata)
    bic = BIC(subdata)
    model = hc.estimate(scoring_method=bic)
    score = bic.score(model)

    results.append((items, list(model.edges()), score))

# 4) Sort & display the top‑K networks
for items, edges, score in sorted(results, key=lambda x: x[2], reverse=True):
    print(f"Itemset {items!s} → edges={edges!s}, BIC={score:.2f}")



freq_itemsets = apriori(df_bool, min_support=0.3, use_colnames=True)
freq_itemsets['length'] = freq_itemsets['itemsets'].apply(len)
top_k = (freq_itemsets[freq_itemsets['length'] >= 2]
         .sort_values('support', ascending=False)
         .head(K))
results = []
for _, row in top_k.iterrows():
    items = list(row['itemsets'])
    subdata = df_bool[items].astype(int)

    hc  = HillClimbSearch(subdata)
    bic = BIC(subdata)
    model = hc.estimate(scoring_method=bic, show_progress=False)
    score = bic.score(model)

    results.append((items, list(model.edges()), score))
for items, edges, score in sorted(results, key=lambda x: x[2], reverse=True):
    print(f"{items} → edges={edges}, BIC={score:.2f}")

import itertools
import heapq
import numpy as np
import pandas as pd

from pgmpy.estimators import HillClimbSearch, BIC
from pgmpy.models import BayesianNetwork
uncertain_db = [
    {'A':0.9, 'B':0.8, 'C':0.1},
    {'A':0.7,        'C':0.6},
    {       'B':0.4, 'C':0.9},
    {'A':0.5, 'B':0.3       },
    {       'B':0.8, 'C':0.7},
]
def expected_support(db, itemset):
    sup = 0.0
    for T in db:
        p = 1.0
        for i in itemset:
            p *= T.get(i, 0.0)
        sup += p
    return sup
def mine_frequent_itemsets(db, sigma):
    items   = sorted({i for T in db for i in T})
    freq    = []
    for L in range(2, len(items)+1):
        for combo in itertools.combinations(items, L):
            es = expected_support(db, combo)
            if es >= sigma:
                freq.append((combo, es))
    return sorted(freq, key=lambda x: -x[1])
def top_k_bayesian_networks(db, sigma, K):
    freq_itemsets = mine_frequent_itemsets(db, sigma)
    print("Frequent itemsets ≥ σ:", freq_itemsets)
    if not freq_itemsets:
        print("→ No itemsets with expected-support ≥", sigma)
        return []

    heap = []
    rng  = np.random.default_rng(0)

    for itemset, esup in freq_itemsets:
        if len(heap)==K and esup < heap[0][0]:
            break
        data = []
        for T in db:
            row = {i: int(rng.random() < T.get(i,0)) for i in itemset}
            data.append(row)
        df = pd.DataFrame(data)
        hc    = HillClimbSearch(df)
        bic   = BIC(df)
        model = hc.estimate(scoring_method=bic, show_progress=False)
        score = bic.score(model)
        if len(heap) < K:
            heapq.heappush(heap, (score, itemset, model))
        elif score > heap[0][0]:
            heapq.heapreplace(heap, (score, itemset, model))

    return sorted(heap, key=lambda x: -x[0])

if __name__ == "__main__":
    SIGMA = 0.8
    K     = 3

    topk = top_k_bayesian_networks(uncertain_db, SIGMA, K)
    if topk:
        print("\nTop‑K Networks:")
        for score, itemset, model in topk:
            print(f"  {itemset} → BIC={score:.2f}, edges={list(model.edges())}")

import itertools

def expected_support(data, itemset):
    support = 0.0
    for transaction in data:
        prob = 1.0
        for item in itemset:
            prob *= transaction.get(item, 0)
        support += prob
    return support / len(data)

def find_frequent_itemsets(data, min_support):
    items = set()
    for transaction in data:
        items.update(transaction.keys())

    items = list(items)
    frequent_itemsets = []
    for size in range(1, len(items)+1):
        for itemset in itertools.combinations(items, size):
            sup = expected_support(data, itemset)
            if sup >= min_support:
                frequent_itemsets.append( (itemset, sup) )

    return frequent_itemsets
data = [
    {'A': 0.9, 'B': 0.8, 'C': 0.1},
    {'A': 0.7, 'B': 0.6},
    {'B': 0.95, 'C': 0.9},
    {'A': 0.8, 'C': 0.2},
]

min_support = 0.5

frequent_itemsets = find_frequent_itemsets(data, min_support)
print("Frequent itemsets ≥ σ:", frequent_itemsets)

import itertools
import math

# ---- PARAMETERS ----
K = 3                          # Top-K networks to keep
min_support = 0.6             # Support threshold
min_confidence = 0.6          # Confidence for creating edges

# ---- SAMPLE UNCERTAIN DATASET ----
# Each transaction is a dict of item probabilities
dataset = [
    {'A': 0.9, 'B': 0.8, 'C': 0.2},
    {'A': 0.7, 'B': 0.1, 'C': 0.6},
    {'A': 0.4, 'B': 0.6, 'C': 0.7},
    {'A': 0.3, 'B': 0.5, 'C': 0.9},
]

# ---- STEP 1: Generate Expected Support ----
def expected_support(itemset, data):
    support = 0.0
    for transaction in data:
        p = 1.0
        for item in itemset:
            p *= transaction.get(item, 0)
        support += p
    return support / len(data)

# ---- STEP 2: Mine Frequent Itemsets ----
def find_frequent_itemsets(data, max_len=3):
    items = set(itertools.chain.from_iterable(d.keys() for d in data))
    frequent_itemsets = []

    for size in range(1, max_len+1):
        for combo in itertools.combinations(items, size):
            sup = expected_support(combo, data)
            if sup >= min_support:
                frequent_itemsets.append((combo, sup))
    return frequent_itemsets

# ---- STEP 3: Build Custom BN from Itemset ----
def build_network(itemset, data):
    edges = []
    itemset = list(itemset)
    n = len(data)

    for a, b in itertools.permutations(itemset, 2):
        count_ab = 0.0
        count_a = 0.0
        for t in data:
            count_a += t.get(a, 0)
            count_ab += t.get(a, 0) * t.get(b, 0)

        p_b_given_a = count_ab / count_a if count_a > 0 else 0
        if p_b_given_a >= min_confidence:
            edges.append((a, b, round(p_b_given_a, 2)))

    return edges

# ---- STEP 4: Score and Rank ----
def score_network(itemset, support, edges):
    edge_score = sum(conf for _, _, conf in edges)
    return round(support + edge_score, 4)

# ---- MAIN EXECUTION ----
frequent_itemsets = find_frequent_itemsets(dataset)
results = []

for items, support in frequent_itemsets:
    edges = build_network(items, dataset)
    score = score_network(items, support, edges)
    results.append({
        'items': items,
        'support': round(support, 4),
        'edges': edges,
        'score': score
    })

# ---- Sort and Take Top-K ----
top_k = sorted(results, key=lambda x: x['score'], reverse=True)[:K]

# ---- Display Results ----
for i, net in enumerate(top_k, 1):
    print(f"\nTop-{i} Network")
    print("Items:", net['items'])
    print("Support:", net['support'])
    print("Edges:")
    for src, dst, conf in net['edges']:
        print(f"  {src} → {dst} (P={conf})")
    print("Score:", net['score'])

import math, heapq


def expected_support(itemset, data):
    """Expected support: sum of products of probabilities of items in each transaction."""
    total = 0.0
    for t in data:
        prod = 1.0
        for item in itemset:
            prod *= t.get(item, 0.0)
        total += prod
    return total

def has_cycle(nodes, edges):
    """Return True if graph has a cycle (False if acyclic)."""
    children = {n: [] for n in nodes}
    for (u,v) in edges:
        children[u].append(v)
    visited, stack = set(), set()
    def dfs(u):
        visited.add(u); stack.add(u)
        for v in children.get(u, []):
            if v not in visited:
                if dfs(v): return True
            elif v in stack:
                return True
        stack.remove(u)
        return False
    for n in nodes:
        if n not in visited:
            if dfs(n):
                return True
    return False

def is_dag(nodes, edges):
    return not has_cycle(nodes, edges)

def compute_score(nodes, edges, data, weight_support):
    """Compute composite score = BIC + weight_support * sum(expected supports of (child+parents) sets)."""
    # Build parent lists from edges
    parents = {n: [] for n in nodes}
    for (u,v) in edges:
        parents[v].append(u)

    # Sum expected supports of each child+parent set
    support_sum = 0.0
    for node in nodes:
        if parents[node]:
            itemset = set(parents[node] + [node])
            support_sum += expected_support(itemset, data)
    for node in nodes:
      if len(parents[node]) > 2:
        return -float("inf")

    # Compute log-likelihood and parameter count for BIC
    loglike = 0.0
    N = len(data)
    param_count = 0
    for node in nodes:
        P = parents[node]
        if P:
            # all binary configs of parents
            for bits in range(2**len(P)):
                total_count = count_X1 = 0.0
                for t in data:
                    prob = 1.0
                    for i,p in enumerate(P):
                        if (bits >> i) & 1:
                            prob *= t.get(p, 0.0)
                        else:
                            prob *= (1 - t.get(p, 0.0))
                    total_count += prob
                    count_X1 += prob * t.get(node, 0.0)
                count_X0 = total_count - count_X1
                if total_count > 0:
                    param_count += 1
                    # add contributions to log-likelihood
                    if count_X1 > 0:
                        loglike += count_X1 * math.log(count_X1/total_count)
                    if count_X0 > 0:
                        loglike += count_X0 * math.log(count_X0/total_count)
        else:
            # Node with no parents
            total_count = float(N)
            count_X1 = sum(t.get(node,0.0) for t in data)
            count_X0 = total_count - count_X1
            if total_count > 0:
                param_count += 1
                if count_X1 > 0:
                    loglike += count_X1 * math.log(count_X1/total_count)
                if count_X0 > 0:
                    loglike += count_X0 * math.log(count_X0/total_count)

    # BIC score: log-likelihood minus (log(N)/2)*param_count (see :contentReference[oaicite:11]{index=11})
    bic = loglike - (math.log(N)/2.0) * param_count

    return bic + weight_support * support_sum

def top_k_bayesian_networks(data, min_support, K, weight_support, sample_size):
    if sample_size < len(data):
        import random
        data = random.sample(data, sample_size)
    """Return top-K BN structures (as edge-sets) by combined support+score."""
    # 1. Collect items and candidate edges (frequent pairs)
    nodes = sorted({ x for t in data for x in t.keys() })
    edges_candidates = []
    for i in range(len(nodes)):
        for j in range(i+1, len(nodes)):
            sup = expected_support({nodes[i], nodes[j]}, data)
            if sup >= min_support:
                edges_candidates.append((nodes[i], nodes[j]))
                edges_candidates.append((nodes[j], nodes[i]))

    # 2. Best-first search initialization
    initial_edges = frozenset()   # start with empty graph
    initial_score = compute_score(nodes, initial_edges, data, weight_support)
    heap = [(-initial_score, initial_edges)]
    visited = set()
    top_networks = []

    # 3. Search loop
    while heap and len(top_networks) < K:
        negscore, edges = heapq.heappop(heap)
        score = -negscore
        if edges in visited:
            continue
        visited.add(edges)
        top_networks.append((edges, score))

        # expand by adding one new edge from candidates
        for e in edges_candidates:
            if e in edges:
                continue
            new_edges = set(edges)
            new_edges.add(e)
            if is_dag(nodes, new_edges):
                new_edges = frozenset(new_edges)
                if new_edges not in visited:
                    new_score = compute_score(nodes, new_edges, data, weight_support)
                    heapq.heappush(heap, (-new_score, new_edges))

    return top_networks

# Example usage
# data = [
#    {"A": 0.1, "B": 0.1},
#    {"A": 0.9, "C": 0.8},
#    {"B": 0.3, "D": 0.4},
#    {"C": 0.9, "D": 0.95},
#    {"B": 0.2, "C": 0.1},
#]
min_support = 0.1
K = 10
weight_support = 0.5
topK = top_k_bayesian_networks(data, min_support, K, weight_support, sample_size=3000)
for edges, score in topK:
    print("Edges:", sorted(edges), "Score:", score)
# remember to build a problem formulation for this.
# there must be a problem before you can build a solution.
!git config --global user.email "giabao120322@gmail.com"
!git config --global user.name "GiaBao12032001"
!git remote set-url origin https://GiaBao12032001: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@github.com/GiaBao12032001/Top-K-Baysian.git
!git init
!git remote add origin https://github.com/GiaBao12032001/Top-K-Baysian
!git add .
!git commit -m "Initial commit: Top-K Bayesian with pruning and sampling"
!git push -u origin master

import matplotlib.pyplot as plt
scores = []
edge_counts = []
for edges, score in topK:
    scores.append(score)
    edge_counts.append(len(edges))
plt.figure(figsize=(8, 5))
plt.plot(edge_counts, scores, 'o-', color='blue')
plt.xlabel('Number of Edges in DAG')
plt.ylabel('Score (BIC + Support)')
plt.title('Top-K Bayesian Networks: Score vs. Network Complexity')
plt.grid(True)
plt.tight_layout()
plt.show()

import time
import random
import matplotlib.pyplot as plt

n_trials = [100, 1000, 10000, 100000, 200000, 300000, 500000]  # number of transactions
times = []

for t in n_trials:
    data = []
    for _ in range(t):
        data.append({
            "A": round(random.uniform(0, 1), 2),
            "B": round(random.uniform(0, 1), 2),
            "C": round(random.uniform(0, 1), 2),
            "D": round(random.uniform(0, 1), 2),
            "A1": round(random.uniform(0, 1), 2),
            "B2": round(random.uniform(0, 1), 2),
            "C3": round(random.uniform(0, 1), 2),
            "D4": round(random.uniform(0, 1), 2),
        })
    start = time.time()
    topK = top_k_bayesian_networks(data, min_support, K, weight_support, sample_size=3000)
    end = time.time()

    times.append(end - start)

# Plot
plt.plot(n_trials, times, 'o-', color='green')
plt.xlabel("Number of Transactions")
plt.ylabel("Runtime (seconds)")
plt.title("Runtime vs. Number of Transactions")
plt.grid(True)
plt.show()

import time
item_counts = [2, 3, 4, 5, 6, 7, 8, 10, 12, 15]
runtime_items = []

num_transactions = 500000

for n in item_counts:
    data = []
    for _ in range(num_transactions):
        txn = {}
        for i in range(n):
            item = chr(ord('A') + i)
            txn[item] = round(random.uniform(0.0, 1.0), 2)
        data.append(txn)

    start = time.time()
    top_k = top_k_bayesian_networks(data, min_support=0.3, K=5, weight_support=0, sample_size=3000)
    end = time.time()

    runtime_items.append(end - start)

# Plot
plt.figure(figsize=(11, 5))
plt.plot(item_counts, runtime_items, 'o-', color='red')
plt.xlabel("Number of Items")
plt.ylabel("Runtime (seconds)")
plt.title("Runtime vs. Number of Items")
plt.grid(True)
plt.tight_layout()
plt.show()

import tracemalloc
import random
import time
import matplotlib.pyplot as plt

item_counts = [2, 3, 4, 5, 6, 7, 8, 10, 12, 15]
mem_usages = []

for n in item_counts:
    # Start memory tracking
    tracemalloc.start()
    topK = top_k_bayesian_networks(data, min_support=0.3, K=5, weight_support=0, sample_size=3000)
    current, peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()

    # Record peak in kilobytes
    mem_usages.append(peak / 1024)

# Plot
plt.plot(item_counts, mem_usages, 'o-', color='purple')
plt.xlabel("Number of Items")
plt.ylabel("Peak Memory Usage (KB)")
plt.title("Space Complexity: Memory vs. Number of Items")
plt.grid(True)
plt.tight_layout()
plt.show()

import random

# Parameters
num_transactions = 500000  # You can increase this to 50_000 or more
item_labels = ["A", "B", "C", "D", "E", "F", "G", "H"]  # 8 items

# Generate dataset
data = []
for _ in range(num_transactions):
    txn = {}
    for item in item_labels:
        # Each item has a probability between 0.0 and 1.0
        txn[item] = round(random.uniform(0.0, 1.0), 2)
    data.append(txn)

# Example usage
print("First 2 transactions:")
print(data[0])
print(data[1])
print(f"Total transactions: {len(data)}")

import matplotlib.pyplot as plt

# List of different min_support values to test
support_values = [0.1, 0.3, 0.5, 0.7]
colors = ['blue', 'green', 'red', 'orange']

plt.figure(figsize=(9, 6))

for i, support in enumerate(support_values):
    # Run your top-K algorithm with each support value
    topK = top_k_bayesian_networks(
        data, min_support=support, K=10, weight_support=0.5, sample_size=3000
    )

    # Prepare data for plotting
    scores = []
    edge_counts = []
    for edges, score in topK:
        scores.append(score)
        edge_counts.append(len(edges))

    # Plot each line with different color and label
    plt.plot(edge_counts, scores, 'o-', color=colors[i], label=f"support = {support}")

# Add labels and title
plt.xlabel('Number of Edges in DAG')
plt.ylabel('Score (BIC + Support)')
plt.title('Top-K Bayesian Networks: Score vs. Network Complexity')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Different weight_support settings to test
weight_values = [0, 0.5, 1.0, 2.0]
colors = ['blue', 'green', 'red', 'orange']

plt.figure(figsize=(9, 6))

for i, w in enumerate(weight_values):
    topK = top_k_bayesian_networks(
        data, min_support=0.3, K=5, weight_support=w, sample_size=3000
    )

    scores = []
    edge_counts = []
    for edges, score in topK:
        scores.append(score)
        edge_counts.append(len(edges))

    plt.plot(edge_counts, scores, 'o-', color=colors[i], label=f"weight = {w}")

# Labels and title
plt.xlabel('Number of Edges in DAG')
plt.ylabel('Score (BIC + Support)')
plt.title('Top-K Bayesian Networks: Score vs. Network Complexity (Weight Effect)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

!git init
!git remote add origin https://github.com/GiaBao12032001/Top-K-Baysian
!git add .
!git commit -m "Initial commit: Top-K Bayesian with pruning and sampling"
!git push -u origin master

!git config --global user.email "giabao120322@gmail.com"
  !git config --global user.name "GiaBao12032001"
  !git remote set-url origin !git remote set-url origin https://GiaBao12032001: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@github.com/GiaBao12032001/Top-K-Baysian.git